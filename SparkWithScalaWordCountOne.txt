start-dfs.sh
start-yarn.sh
start-master.sh
start-slave.sh spark://thanoojubuntu-Inspiron-3521:7077
start-history-server.sh

hduser@thanoojubuntu-Inspiron-3521:~$ jps
17344 DataNode
17168 NameNode
17809 ResourceManager
42515 HistoryServer
17572 SecondaryNameNode
27446 Master
22567 XMLServerLauncher
17961 NodeManager
22123 org.eclipse.equinox.launcher_1.5.700.v20200207-2156.jar
27519 Worker
43039 Jps
hduser@thanoojubuntu-Inspiron-3521:~$ 

hduser@thanoojubuntu-Inspiron-3521:~$ hdfs dfs -put wordcount.txt hive/inputs/wc/



hduser@thanoojubuntu-Inspiron-3521:~$ hdfs dfs -ls hive/inputs
Found 2 items
drwxr-xr-x   - hduser hadoop          0 2020-10-28 02:09 hive/inputs/accounts
drwxr-xr-x   - hduser hadoop          0 2020-11-07 02:38 hive/inputs/wc
hduser@thanoojubuntu-Inspiron-3521:~$ hdfs dfs -ls hive/inputs/wc
Found 1 items
-rw-r--r--   1 hduser hadoop        744 2020-11-07 02:38 hive/inputs/wc/wordcount.txt
hduser@thanoojubuntu-Inspiron-3521:~$ hdfs dfs -cat hive/inputs/wc/wordcount.txt
These examples give a quick overview of the Spark API. Spark is built on the concept of distributed datasets, which contain arbitrary Java or Python objects. You create a dataset from external data, then apply parallel operations to it. The building block of the Spark API is its RDD API. In the RDD API, there are two types of operations: transformations, which define a new dataset based on previous ones, and actions, which kick off a job to execute on a cluster. On top of Sparkâ€™s RDD API, high level APIs are provided, e.g. DataFrame API and Machine Learning API. These high level APIs provide a concise way to conduct certain data operations. In this page, we will show examples using RDD API as well as examples using high level APIs.
hduser@thanoojubuntu-Inspiron-3521:~$ 



package com

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions
import java.util.Date
import java.util.Calendar

object WordCountOne {
  def main(args: Array[String]): Unit = {
    val sparkConf = new SparkConf();
    sparkConf.setMaster("spark://thanoojubuntu-Inspiron-3521:7077")
    sparkConf.setAppName("SparkWithScalaWordCount")
    val sc = new SparkContext(sparkConf)
    val textFile = sc.textFile("hdfs://localhost:9000/user/hduser/hive/inputs/wc")
    val wordCounts = textFile.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
    wordCounts.saveAsTextFile("hdfs://localhost:9000/user/hduser/hive/outputs/wc_"+Calendar.getInstance.getTimeInMillis)
    sc.stop()
  }
}



$ mvn package

hduser@thanoojubuntu-Inspiron-3521:/$ spark-shell --class "com.WordCountOne" --master spark://thanoojubuntu-Inspiron-3521:7077 --jars /home/hduser/eclipse-workspace/SparkWithScalaWSs/One/SparkWithScala/target/SparkWithScala-0.0.1-SNAPSHOT.jar
20/11/07 03:33:27 WARN util.Utils: Your hostname, thanoojubuntu-Inspiron-3521 resolves to a loopback address: 127.0.1.1; using 192.168.29.128 instead (on interface wlp8s0)
20/11/07 03:33:27 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/11/07 03:33:28 INFO spark.SparkContext: Running Spark version 2.4.6
20/11/07 03:33:28 INFO spark.SparkContext: Submitted application: SparkWithScalaWordCount
20/11/07 03:33:28 INFO spark.SecurityManager: Changing view acls to: hduser
20/11/07 03:33:28 INFO spark.SecurityManager: Changing modify acls to: hduser
20/11/07 03:33:28 INFO spark.SecurityManager: Changing view acls groups to: 
20/11/07 03:33:28 INFO spark.SecurityManager: Changing modify acls groups to: 
20/11/07 03:33:28 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hduser); groups with view permissions: Set(); users  with modify permissions: Set(hduser); groups with modify permissions: Set()
20/11/07 03:33:28 INFO util.Utils: Successfully started service 'sparkDriver' on port 44093.
20/11/07 03:33:28 INFO spark.SparkEnv: Registering MapOutputTracker
20/11/07 03:33:28 INFO spark.SparkEnv: Registering BlockManagerMaster
20/11/07 03:33:28 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/07 03:33:28 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/07 03:33:28 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-72eb4b43-0c12-4b75-b3af-46a7647dbed6
20/11/07 03:33:29 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB
20/11/07 03:33:29 INFO spark.SparkEnv: Registering OutputCommitCoordinator
20/11/07 03:33:29 INFO util.log: Logging initialized @2859ms
20/11/07 03:33:29 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown
20/11/07 03:33:29 INFO server.Server: Started @2989ms
20/11/07 03:33:29 INFO server.AbstractConnector: Started ServerConnector@400d912a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/11/07 03:33:29 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@c68a5f8{/jobs,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29a23c3d{/jobs/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b6ac111{/jobs/job,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@591fd34d{/jobs/job/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@61e45f87{/stages,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c9b78e3{/stages/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3068b369{/stages/stage,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@736ac09a{/stages/stage/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ecd665{/stages/pool,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@45394b31{/stages/pool/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ec7d8b3{/storage,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b0ca5e1{/storage/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb3131b{/storage/rdd,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54dcbb9f{/storage/rdd/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74fef3f7{/environment,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a037324{/environment/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69eb86b4{/executors,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@585ac855{/executors/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5bb8f9e2{/executors/threadDump,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6a933be2{/executors/threadDump/json,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f78de22{/static,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2189e7a7{/,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69b2f8e5{/api,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a411233{/jobs/job/kill,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70325d20{/stages/stage/kill,null,AVAILABLE,@Spark}
20/11/07 03:33:29 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.29.128:4040
20/11/07 03:33:29 INFO spark.SparkContext: Added JAR file:///home/hduser/eclipse-workspace/SparkWithScalaWSs/One/SparkWithScala/target/SparkWithScala-0.0.1-SNAPSHOT.jar at spark://192.168.29.128:44093/jars/SparkWithScala-0.0.1-SNAPSHOT.jar with timestamp 1604700209454
20/11/07 03:33:29 INFO client.StandaloneAppClient$ClientEndpoint: Connecting to master spark://thanoojubuntu-Inspiron-3521:7077...
20/11/07 03:33:29 INFO client.TransportClientFactory: Successfully created connection to thanoojubuntu-Inspiron-3521/127.0.1.1:7077 after 55 ms (0 ms spent in bootstraps)
20/11/07 03:33:29 INFO cluster.StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20201107033329-0014
20/11/07 03:33:29 INFO client.StandaloneAppClient$ClientEndpoint: Executor added: app-20201107033329-0014/0 on worker-20201107010804-192.168.29.128-46823 (192.168.29.128:46823) with 2 core(s)
20/11/07 03:33:29 INFO cluster.StandaloneSchedulerBackend: Granted executor ID app-20201107033329-0014/0 on hostPort 192.168.29.128:46823 with 2 core(s), 1024.0 MB RAM
20/11/07 03:33:29 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42083.
20/11/07 03:33:29 INFO netty.NettyBlockTransferService: Server created on 192.168.29.128:42083
20/11/07 03:33:29 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/07 03:33:29 INFO client.StandaloneAppClient$ClientEndpoint: Executor updated: app-20201107033329-0014/0 is now RUNNING
20/11/07 03:33:30 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.29.128, 42083, None)
20/11/07 03:33:30 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.29.128:42083 with 366.3 MB RAM, BlockManagerId(driver, 192.168.29.128, 42083, None)
20/11/07 03:33:30 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.29.128, 42083, None)
20/11/07 03:33:30 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.29.128, 42083, None)
20/11/07 03:33:30 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c6aa04a{/metrics/json,null,AVAILABLE,@Spark}
20/11/07 03:33:30 INFO scheduler.EventLoggingListener: Logging events to file:/tmp/spark-events/app-20201107033329-0014
20/11/07 03:33:31 INFO cluster.StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
20/11/07 03:33:31 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 240.3 KB, free 366.1 MB)
20/11/07 03:33:31 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.2 KB, free 366.0 MB)
20/11/07 03:33:31 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.29.128:42083 (size: 23.2 KB, free: 366.3 MB)
20/11/07 03:33:31 INFO spark.SparkContext: Created broadcast 0 from textFile at WordCountOne.scala:15
20/11/07 03:33:32 INFO mapred.FileInputFormat: Total input paths to process : 1
20/11/07 03:33:32 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
20/11/07 03:33:32 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
20/11/07 03:33:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1
20/11/07 03:33:32 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Registering RDD 3 (map at WordCountOne.scala:17) as input to shuffle 0
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (runJob at SparkHadoopWriter.scala:78)
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCountOne.scala:17), which has no missing parents
20/11/07 03:33:33 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.1 KB, free 366.0 MB)
20/11/07 03:33:33 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.0 KB, free 366.0 MB)
20/11/07 03:33:33 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.29.128:42083 (size: 3.0 KB, free: 366.3 MB)
20/11/07 03:33:33 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163
20/11/07 03:33:33 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at WordCountOne.scala:17) (first 15 tasks are for partitions Vector(0, 1))
20/11/07 03:33:33 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
20/11/07 03:33:33 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.29.128:42554) with ID 0
20/11/07 03:33:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.29.128, executor 0, partition 0, ANY, 7915 bytes)
20/11/07 03:33:33 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, 192.168.29.128, executor 0, partition 1, ANY, 7915 bytes)
20/11/07 03:33:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager 192.168.29.128:42611 with 366.3 MB RAM, BlockManagerId(0, 192.168.29.128, 42611, None)
20/11/07 03:33:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.29.128:42611 (size: 3.0 KB, free: 366.3 MB)
20/11/07 03:33:34 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.29.128:42611 (size: 23.2 KB, free: 366.3 MB)
20/11/07 03:33:36 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2541 ms on 192.168.29.128 (executor 0) (1/2)
20/11/07 03:33:36 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2630 ms on 192.168.29.128 (executor 0) (2/2)
20/11/07 03:33:36 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/07 03:33:36 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (map at WordCountOne.scala:17) finished in 3.021 s
20/11/07 03:33:36 INFO scheduler.DAGScheduler: looking for newly runnable stages
20/11/07 03:33:36 INFO scheduler.DAGScheduler: running: Set()
20/11/07 03:33:36 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)
20/11/07 03:33:36 INFO scheduler.DAGScheduler: failed: Set()
20/11/07 03:33:36 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at WordCountOne.scala:19), which has no missing parents
20/11/07 03:33:36 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.8 KB, free 366.0 MB)
20/11/07 03:33:36 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 26.5 KB, free 365.9 MB)
20/11/07 03:33:36 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.29.128:42083 (size: 26.5 KB, free: 366.2 MB)
20/11/07 03:33:36 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163
20/11/07 03:33:36 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at saveAsTextFile at WordCountOne.scala:19) (first 15 tasks are for partitions Vector(0, 1))
20/11/07 03:33:36 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
20/11/07 03:33:36 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, 192.168.29.128, executor 0, partition 0, NODE_LOCAL, 7666 bytes)
20/11/07 03:33:36 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, 192.168.29.128, executor 0, partition 1, NODE_LOCAL, 7666 bytes)
20/11/07 03:33:36 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.29.128:42611 (size: 26.5 KB, free: 366.2 MB)
20/11/07 03:33:36 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.29.128:42554
20/11/07 03:33:37 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 1061 ms on 192.168.29.128 (executor 0) (1/2)
20/11/07 03:33:37 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 1056 ms on 192.168.29.128 (executor 0) (2/2)
20/11/07 03:33:37 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/07 03:33:37 INFO scheduler.DAGScheduler: ResultStage 1 (runJob at SparkHadoopWriter.scala:78) finished in 1.144 s
20/11/07 03:33:37 INFO scheduler.DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 4.810672 s
20/11/07 03:33:37 INFO io.SparkHadoopWriter: Job job_20201107033332_0005 committed.
20/11/07 03:33:37 INFO server.AbstractConnector: Stopped Spark@400d912a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
20/11/07 03:33:37 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.29.128:4040
20/11/07 03:33:37 INFO cluster.StandaloneSchedulerBackend: Shutting down all executors
20/11/07 03:33:37 INFO cluster.CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
20/11/07 03:33:37 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/07 03:33:37 INFO memory.MemoryStore: MemoryStore cleared
20/11/07 03:33:37 INFO storage.BlockManager: BlockManager stopped
20/11/07 03:33:37 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
20/11/07 03:33:37 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/07 03:33:37 INFO spark.SparkContext: Successfully stopped SparkContext
20/11/07 03:33:37 INFO util.ShutdownHookManager: Shutdown hook called
20/11/07 03:33:37 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b98c6228-e4b1-4d26-abd8-9c4042a81a6c
20/11/07 03:33:37 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ba21f512-a7fa-4ea6-bc04-8b6749283eee
hduser@thanoojubuntu-Inspiron-3521:/$ 

hduser@thanoojubuntu-Inspiron-3521:~$ hdfs dfs -ls hive/outputs
Found 1 items
drwxr-xr-x   - hduser hadoop          0 2020-11-07 03:04 hive/outputs/wc_1604698462639
hduser@thanoojubuntu-Inspiron-3521:~$ hdfs dfs -ls hive/outputs/wc_1604698462639
Found 3 items
-rw-r--r--   1 hduser hadoop          0 2020-11-07 03:04 hive/outputs/wc_1604698462639/_SUCCESS
-rw-r--r--   1 hduser hadoop        456 2020-11-07 03:04 hive/outputs/wc_1604698462639/part-00000
-rw-r--r--   1 hduser hadoop        453 2020-11-07 03:04 hive/outputs/wc_1604698462639/part-00001



